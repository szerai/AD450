{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7 Overview\n",
    "\n",
    "Again this week will be a mix of data processing and linear algebra. \n",
    "The first 5 problems are data cleaning and the final two problems are linear algebra. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Techniques for cleaning & preparing data\n",
    "\n",
    "### Learning Outcomes\n",
    "- how to find & fix missing values\n",
    "- how to simplify your data\n",
    "- how to fix data-type problems\n",
    "- how to work with indexes\n",
    "\n",
    "\n",
    "Data cleaning and transformation is one of the situations where importing from a python file is helpful. For example in this [project](https://github.com/hlandrus/olympic_climbing_prediction) I created a [notebook](https://github.com/hlandrus/olympic_climbing_prediction/blob/main/explore-ifsc-data.ipynb) that had most of my model building and testing while the file [data_processing.py](https://github.com/hlandrus/olympic_climbing_prediction/blob/main/data_processing.py) contained the data processing required clean and transform the data. I did this because the data processing doesn't add understanding to the project. Instead I focused on keeping the interesting things in the notebook and the less interesting things in a python file. You could do this with many things like putting a model training and testing pipeline into a file and only running and displaying the results in your notebook. Overall think about putting what you want to communicate and share in the notebook and what is not helpful for communicating your findings into a file.\n",
    "\n",
    "\n",
    "## Important\n",
    "We will start using this paradigm in this notebook. I have already written all the code in this notebook that will call functions you will create in `data_processing.py`. Once you have completed that you should be able to run this notebook and get the desired results. \n",
    "\n",
    "I would encourage you to play around with processing the data in this notebook and then once you have the code figured over move it to `data_processing.py`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "\n",
    "Cleaning data is a crucial and often time-consuming step in data science.\n",
    "\n",
    "Data scientists might use pure Python, pandas, or other programming tools for this step. Examples here focus on pandas with a few other approaches for specific scenarios.\n",
    "\n",
    "Common tasks are:\n",
    "\n",
    "*   Handling missing data\n",
    "*   Cleaning up column names\n",
    "*   Simplifying data\n",
    "*   Data-type conversion\n",
    "\n",
    "We will be using the csv `messy_IMDB_dataset.csv` imported below. Notice the seperator and encoding values are specified here and not the default. This is because this is a fairly non-standard csv using `;` as seperators and an encoding of `latin-1`. Take a look at the raw csv to see how this data looks different from other csvs you have seen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import importlib\n",
    "import data_processing\n",
    "\n",
    "df_raw = pd.read_csv('messy_IMDB_dataset.csv', sep=\";\" , encoding='latin-1')\n",
    "# Starting stats of data\n",
    "df_raw.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preping Columns\n",
    "\n",
    "When you get data from unknown sources they often have columne names that are difficult to work with. For example it is harder to work with with a column named `IMBD title ID` than `imbd_title_id` because you would have to remember which letters where upper case and double clicking the column name doesn't automatically highlight the full colum name in case you wanted to copy and paste it. This data also has non-standard characters in the column name. \n",
    "\n",
    "### Problem 1 (5 pts):\n",
    "\n",
    "For this first problem you will update the column names as follows:\n",
    "* All lower case\n",
    "* Remove trailing and leading spaces\n",
    "* Turn spaces between words to underscores\n",
    "* Specific Changes: \n",
    "    * Original titlÊ -> original_title\n",
    "    * Genrë¨ -> genre\n",
    "    * Unnamed:_8 -> unnamed_8\n",
    "\n",
    "To do this create a function in `data_processing.py` called `rename_columns`. This function should take in `df_raw` and return a new dataframe with the renamed columns. Save that new dataframe to a variable in your notebook called `df_columns_renamed`. If you want to pass other information into `rename_columns` feel free.\n",
    "\n",
    "This patterned is displayed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_processing import rename_columns\n",
    "df_columns_renamed = rename_columns(df_raw)\n",
    "df_columns_renamed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Missing Data\n",
    "\n",
    "Often data analysts need to account for missing data values.\n",
    "\n",
    "pandas uses the floating-point value `NaN` (Not a Number) to represent missing numerica data. This is a `sentinel` value that can be easily detected.\n",
    "\n",
    "The built-in Python `None` value is also treated as NaN.\n",
    "\n",
    "pandas has several methods for detecting NaN values in a Series or DataFrame:\n",
    "- isnull\n",
    "- notnull\n",
    "\n",
    "These methods can be used as filters in a data query.\n",
    "\n",
    "`data[data.notnull()]`\n",
    "\n",
    "Alternatively, programs can use `dropna` to filter axis labels where values may have missing data.\n",
    "\n",
    "`dropna` has options to control how many missing values a row or column should have to be dropped.\n",
    "\n",
    "**replace missing values**\n",
    "Sometimes it's more useful to replace missing data with a specific or interpreted value, using `fillna`.\n",
    "\n",
    "```\n",
    "df_2 = df.fillna(-1) # return new dataframe with -1 in place of missing values\n",
    "df.fillna(-1, inplace=True) # fill missing values with -1 in original dataframe\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2 (10 pts):\n",
    "\n",
    "The first thing we are going to do is remove completely null rows and columns. To do this create a function in `data_processing.py` called `remove_fully_null_columns_rows`. This function should take in `df_columns_renamed` and return a new dataframe with the renamed columns. Save that new dataframe to a variable in your notebook called `df_fully_null_removed`.\n",
    "\n",
    "Once this is done `df_fully_null_removed` should have shape `(100, 11)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_processing)\n",
    "from data_processing import remove_fully_null_columns_rows\n",
    "\n",
    "df_fully_null_removed = remove_fully_null_columns_rows(df_columns_renamed)\n",
    "df_fully_null_removed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3 (10 pts):\n",
    "Looking at describe again we can see that most of our columns *appear* not null except `duration` and `content_rating`. We are going to ignore `duration` for now and focus on `content_rating`. \n",
    "\n",
    "In the file `data_processing` you are going to create a function named `clean_and_fill_content_rating`. This function will do the following:\n",
    "* Take in `df_fully_null_removed`\n",
    "* Output a new dataframe with: \n",
    "    * NaN in `content_rating` filled with `Unrated` \n",
    "    * `Not Rated` replaced to `Unrated`\n",
    "\n",
    "Save the output to a new dataframe named `df_clean_content_rating`\n",
    "\n",
    "Your final value counts for `content_rating` should be:\n",
    "\n",
    "```\n",
    "content_rating\n",
    "R           45\n",
    "Unrated     25\n",
    "PG-13       12\n",
    "PG          11\n",
    "G            6\n",
    "Approved     1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_processing)\n",
    "from data_processing import clean_and_fill_content_rating\n",
    "\n",
    "df_clean_content_rating = clean_and_fill_content_rating(df_fully_null_removed)\n",
    "df_clean_content_rating['content_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before we deal with the rest of the null values we are going to do some type conversion because that will lead to move nulls. If you run the cell below you should see that all columns are currently objects but several of them should be converted. \n",
    "* release_year -> date\n",
    "* duration -> int\n",
    "* income -> int\n",
    "* vote -> float\n",
    "* score -> float\n",
    "\n",
    "The unfortunate thing is that if we just try to do any of the straightforward type conversions with pandas these will fail because they have bad data in them. So we will need to fix each column before we convert it. \n",
    "\n",
    "We will only clean `release_year` and `income` in this notebook but if you want more practice feel free to work through the other columns as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_content_rating.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4 (10 pts):\n",
    "For `release_year` you are going to create a function `clean_release_year` in your file. This function will take in `df_clean_content_rating` and output a new dataframe with two new columns: \n",
    "* `release_year_coerce`: Use to use pd.to_datetime with errors='coerce'\n",
    "* `release_year_mixed`: Use to use `pd.to_datetime` with `errors='coerce'` and `format=\"mixed\"`\n",
    "\n",
    "Save the output to `df_clean_release_year_temp`. Then run the next cell to see the results. Manually inspect to see if all the movies have correct release dates. Ideally we would go through and update all the incorrect or missing data but for now we will just remove it. Create a new dataframe `df_clean_release_year` that removed the 4 movies below from `df_clean_release_year_temp`. \n",
    "\n",
    "```\n",
    "['Casablanca', 'Sunset Blvd.', 'Scarface', 'Taxi Driver']\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_processing)\n",
    "from data_processing import clean_release_year\n",
    "\n",
    "\n",
    "df_clean_release_year_temp = clean_release_year(df_clean_content_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_release_year_temp[df_fully_null_removed['release_year_coerce'].isna()\n",
    "                      ][['release_year', 'release_year_coerce', \n",
    "                         'release_year_mixed', 'original_title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove movies here ['Casablanca', 'Sunset Blvd.', 'Scarface', 'Taxi Driver']\n",
    "df_clean_release_year = df_clean_release_year_temp[~df_clean_release_year_temp['original_title'].isin(['Casablanca', 'Sunset Blvd.', 'Scarface', 'Taxi Driver'])].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 (10 pts):\n",
    "\n",
    "The column `income` should be an int but it has `$`, commas, and a few other issues. Create the function   `clean_income` in your file such that it outputs a dataframe with the column `income` correctly cleaned and converted to an int. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data_processing)\n",
    "from data_processing import clean_income\n",
    "\n",
    "df_clean_income = clean_income(df_clean_release_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_income.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Time\n",
    "\n",
    "Here are some problems related to what we learned this week about linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6 (20 pts):\n",
    "\n",
    "<img src=\"images/matrix_transformations_1.png\" width=\"200\"/>\n",
    "<img src=\"images/matrix_transformations_3.png\" width=\"200\"/>\n",
    "<img src=\"images/matrix_transformations_2.png\" width=\"200\"/>\n",
    "<img src=\"images/matrix_transformations_4.png\" width=\"200\"/>\n",
    "\n",
    "\n",
    "1. Create all of the matrices in the images above. If there is a k select a integer to input in.  \n",
    "2. Create two random 2d vectors with integer values less than 10. \n",
    "3. Apply each of the matrices to the vectors. \n",
    "4. Graph the original vectors and the transformed vectors by modifying the quiver code below. If you want to get more creative feel free.  \n",
    "5. For each graph set the title to be a description of the transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "quiver = plt.quiver([0, 0], # Sets the x-cordinate of the start of the vectors\n",
    "                    [0, 0], # Sets the y-cordinate of the start of the vectors\n",
    "                    [1, -1], # Sets the x-cordinate of the end of the vectors\n",
    "                    [1, -1], # Sets the y-cordinate of the end of the vectors\n",
    "           angles='xy', \n",
    "           scale_units='xy', \n",
    "           scale=1,\n",
    "           color=['r','b'])\n",
    "plt.xlim(-2, 2)\n",
    "plt.ylim(-2, 2)\n",
    "plt.quiverkey(quiver, 1, 1, 1, 'orginal vector', coordinates='data')\n",
    "plt.quiverkey(quiver, -1, -1.5, 1, 'transformed vector', coordinates='data')\n",
    "plt.title('Description of Transformation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7 (20 pts):\n",
    "Confirm the LIVE EVIL rule using the following five steps: \n",
    "\n",
    "1. Create four matrices of random numbers, setting the sizes to be $L \\in \\mathbb{R}^{2 \\times 6}$, $I \\in \\mathbb{R}^{6 \\times 3}$, $V \\in \\mathbb{R}^{3 \\times 5}$, $E \\in \\mathbb{R}^{5 \\times 2}$\n",
    "\n",
    "2. Multiply the four matrices and transpose the product. \n",
    "\n",
    "3. Transpose each matrix individually and multiply them without reversing their order. \n",
    "\n",
    "4. Transpose each matrix individually and multiply them reversing their order according to the LIVE EVIL rule. Check whether the result of step 2 matches the results of step 3 and step 4. \n",
    "\n",
    "5. Repeat the previous steps but using all square matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AD_450_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
